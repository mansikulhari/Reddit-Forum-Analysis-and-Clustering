# -*- coding: utf-8 -*-
"""DSCI565_Lab4-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eXrPNqtMXKAKpLwSomuEgo2lzmBypoYJ
"""

!pip3 install praw
import praw
import pandas as pd
import time
from gensim.scripts.glove2word2vec import glove2word2vec
from gensim.models import KeyedVectors
import numpy as np
import urllib.request
import zipfile
import nltk
from nltk.stem import WordNetLemmatizer
import re
import os

reddit = praw.Reddit(client_id='yzjwJGWRC-Ej-ibYbQ5fAg',
                     client_secret='YmuI4saCK99aLecdtd51xYckDSGCFA',
                     user_agent='windows:DSCI560:v1 (by /u/TheDemonicJay)')

subreddit = reddit.subreddit('tech')

# scrape_reddit.py

def top_posts(duration='month', num_posts=60):  # "all", "day", "hour", "month", "week", or "year"
    # Define the batch size and delay as per Reddit's API constraints
    BATCH_SIZE = 60
    DELAY = 65

    posts_dict = {"ID": [], "Title": [], "Author": [], "Post Text": [], "Score": [],
                  "Upvote ratio": [], "Total Comments": [], "Post URL": [], "Media": []}

    count = 0
    iter = 0
    while count < num_posts:
        try:
            posts = subreddit.top(duration, limit=min(BATCH_SIZE, num_posts - count))
            for post in posts:
                if count >= num_posts:
                    break

                posts_dict["Title"].append(post.title)
                posts_dict["Author"].append(post.author.name if post.author else "Deleted")
                posts_dict["Post Text"].append(post.selftext)
                posts_dict["ID"].append(post.id)
                posts_dict["Score"].append(post.score)
                posts_dict["Upvote ratio"].append(post.upvote_ratio)
                posts_dict["Total Comments"].append(post.num_comments)
                posts_dict["Post URL"].append(post.url)
                posts_dict["Media"].append(post.media)

                count += 1
            iter += 1
            # If we haven't reached the desired number of posts, sleep before the next request
            if count < num_posts and count // iter >= 60:
                time.sleep(DELAY)

        except Exception as e:
            print(f"An error occurred: {e}")
            time.sleep(DELAY)

    return pd.DataFrame(posts_dict)


def comments(post_ids):
    comments_dict = {"postId": [], "ID": [], "Body": [], "Author": [], "Score": [], "Total replies": []}
    try:
        for post_id in post_ids:
            post = reddit.submission(id=post_id)
            for comment in post.comments:
                if isinstance(comment, praw.models.MoreComments):
                    continue
                if comment.body == '[deleted]' or not comment.author:
                    continue
                comments_dict["postId"].append(post_id)
                comments_dict["Body"].append(comment.body)
                comments_dict["Author"].append(comment.author.name if comment.author else "Deleted")
                comments_dict["ID"].append(comment.id)
                comments_dict["Score"].append(comment.score)
                comments_dict["Total replies"].append(len(comment.replies))

    except Exception as e:
        print(f"An error occurred: {e}")
        return pd.DataFrame()

    return pd.DataFrame(comments_dict)

# embeddings.py

def get_glove_model():
    glove_file = 'data/glove.6B.100d.txt'
    word2vec_output_file = 'data/glove.6B.100d.txt.word2vec'
    glove2word2vec(glove_file, word2vec_output_file)
    glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)
    return glove_model


def download_glove(directory='data'):
    try:
        os.makedirs(directory)
    except FileExistsError:
        pass

    url = 'http://nlp.stanford.edu/data/glove.6B.zip'
    file_location = os.path.join(directory, 'glove.6B.zip')

    # Download the GloVe model
    urllib.request.urlretrieve(url, file_location)

    # Extract and then delete the zip file
    with zipfile.ZipFile(file_location, 'r') as zip_ref:
        zip_ref.extractall(directory)

    os.remove(file_location)


def get_glove_embedding(sentence: str, model) -> np.ndarray:
    words = sentence.split()
    words = [word for word in words if word in model]  # Only consider words that are in the GloVe model
    if len(words) == 0:
        return np.zeros(model.vector_size)  # If no words are in the model, return a zero vector
    embedding = np.mean([model[word] for word in words], axis=0)
    return embedding


def preprocess_text(text):
    text = re.sub(r'\W', ' ', str(text))  # Remove all the special characters
    text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text)  # Remove all single characters
    text = re.sub(r'\^[a-zA-Z]\s+', ' ', text)  # Remove single characters from the start
    text = re.sub(r'\s+', ' ', text, flags=re.I)  # Substituting multiple spaces with single space
    text = text.lower()  # Converting to Lowercase
    text = re.sub(r'^b\s+', '', text)  # Removing prefixed 'b'

    lemmatizer = WordNetLemmatizer()  # Initializing WordNetLemmatizer
    text = text.split()
    text = [lemmatizer.lemmatize(word) for word in text]
    text = ' '.join(text)

    return text



download_glove() # only run once, it takes awhile
nltk.download('stopwords')
nltk.download('wordnet')

# mysql_utils.py

!pip3 install mysql-connector-python
import mysql.connector
from mysql.connector import Error

def create_connection():
    connection = None
    try:
        # TODO
        # change these arguments to match your own MySQL DB
        connection = mysql.connector.connect(
            host="localhost",
            user="root",
            password="easy",
            database="reddit"
        )

        if connection.is_connected():
            print("Connection to MySQL DB successful")
            cursor = connection.cursor()

            # Modify the table creation queries to include a 'Vector' column
            cursor.execute("""
            CREATE TABLE IF NOT EXISTS Posts (
                ID INT AUTO_INCREMENT PRIMARY KEY,
                Score INT,
                TotalComments INT,
                Vector TEXT  # Add a Text type column to store the vector
            )
            """)

            cursor.execute("""
            CREATE TABLE IF NOT EXISTS Comments (
                ID INT AUTO_INCREMENT PRIMARY KEY,
                Score INT,
                TotalReplies INT,
                Vector TEXT
            )
            """)

            cursor.execute("""
            CREATE TABLE IF NOT EXISTS Keywords (
                KeywordID INT AUTO_INCREMENT PRIMARY KEY,
                Keyword VARCHAR(255) UNIQUE
            )
            """)

            cursor.execute("""
            CREATE TABLE IF NOT EXISTS PostKeywords (
                PostID INT,
                KeywordID INT,
                FOREIGN KEY (PostID) REFERENCES Posts (ID),
                FOREIGN KEY (KeywordID) REFERENCES Keywords (KeywordID)
            )
            """)

            cursor.execute("""
            CREATE TABLE IF NOT EXISTS CommentKeywords (
                CommentID INT,
                PostID VARCHAR(20) NOT NULL,
                KeywordID INT,
                FOREIGN KEY (CommentID) REFERENCES Comments (ID),
                FOREIGN KEY (KeywordID) REFERENCES Keywords (KeywordID)
            )
            """)

            print("Tables checked/created successfully")

    except Error as e:
        print(f"The error '{e}' occurred")

    return connection



def execute_query(connection, query, data=None):
    cursor = connection.cursor()
    try:
        if data:
            cursor.execute(query, data)
        else:
            cursor.execute(query)
        connection.commit()
    except Error as e:
        print(f"The error '{e}' occurred")



def insert_post_metadata(connection, post, vector):
    query = """INSERT INTO Posts (Score, TotalComments, Vector) VALUES (%s, %s, %s)"""
    data = (post.get('Score', 0), post.get('TotalComments', 0), vector)
    execute_query(connection, query, data)
    cursor = connection.cursor()
    cursor.execute("SELECT LAST_INSERT_ID();")
    return cursor.fetchone()[0]


def insert_comment_metadata(connection, comment, vector):
    query = """INSERT INTO Comments (Score, TotalReplies, Vector) VALUES (%s, %s, %s)"""
    data = (comment.get('Score', 0), comment.get('TotalReplies', 0), vector)
    execute_query(connection, query, data)
    cursor = connection.cursor()
    cursor.execute("SELECT LAST_INSERT_ID();")
    return cursor.fetchone()[0]


def insert_keyword_get_id(connection, keyword):
    cursor = connection.cursor()
    query = """INSERT INTO Keywords (Keyword) VALUES (%s) ON DUPLICATE KEY UPDATE KeywordID=LAST_INSERT_ID(KeywordID), Keyword=VALUES(Keyword)"""
    data = (keyword,)
    execute_query(connection, query, data)
    cursor.execute("SELECT LAST_INSERT_ID();")  # Explicitly selecting the last insert id
    return cursor.fetchone()[0]  # Fetching the id as the first element of the first row of results



def insert_into_junction_table(connection, table_name, post_id, id, keyword_id):
    data = (id, post_id, keyword_id)
    query = f"""INSERT INTO {table_name} VALUES (%s, %s, %s)"""
    execute_query(connection, query, data)


def comment_id_exists(connection, comment_id):
    cursor = connection.cursor()
    query = "SELECT CommentID FROM Comments WHERE CommentID = %s"
    cursor.execute(query, (comment_id,))
    return cursor.fetchone() is not None

# from scrape_reddit import top_posts, comments
# from embeddings import get_glove_embedding, preprocess_text, get_glove_model
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import string
# from mysql_utils import create_connection, insert_post_metadata, insert_comment_metadata, \
#         insert_keyword_get_id, execute_query, insert_into_junction_table, comment_id_exists



def extract_keywords(text, num_keywords=10):
    # Tokenize the text into words
    words = word_tokenize(text)

    # Remove punctuation and convert to lower case
    words = [''.join(c for c in w if c not in string.punctuation) for w in words]
    words = [word.lower() for word in words if word]  # remove empty strings

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Count the frequency of each word
    word_freq = Counter(words)

    # Extract top num_keywords keywords
    keywords = word_freq.most_common(num_keywords)

    return keywords


def create_comment_keywords_df(comments_df):
    comment_keywords_df = pd.DataFrame()
    comments_df['postId'] = comments_df['postId'].astype(str)
    comment_keywords_df['Comment ID'] = comments_df['ID']
    comment_keywords_df['Score'] = comments_df['Score']
    comment_keywords_df['Total Replies'] = comments_df['Total replies']
    comment_keywords_df['Keywords'] = comments_df['Body'].apply(extract_keywords)

    return comment_keywords_df


def create_post_keywords_df(top_posts_df):
    post_keywords_df = pd.DataFrame()
    post_keywords_df['Date'] = top_posts_df.index
    post_keywords_df['Post ID'] = top_posts_df['ID']
    post_keywords_df['Score'] = top_posts_df['Score']
    post_keywords_df['Total Comments'] = top_posts_df['Total Comments']
    post_keywords_df['Keywords'] = top_posts_df['Post Text'].apply(extract_keywords)

    return post_keywords_df


def main():
    # Get the top posts from the subreddit
    print('GETTING TOP POSTS')
    posts_df = top_posts()

    # Get the comments for each post
    print('GETTING COMMENTS')
    comments_df = comments(posts_df['ID'])

    # Save the data to a CSV file
    posts_df.to_csv('data/posts.csv', index=False)
    comments_df.to_csv('data/comments.csv', index=False)

    # Get the GloVe model
    print('GETTING GLOVE MODEL')
    glove_model = get_glove_model()

    # add embedding column to comments df by applying get_glove_embedding
    print('ADDING EMBEDDING COLUMN TO COMMENTS DF')
    comments_df['Body'] = comments_df['Body'].apply(preprocess_text)
    comments_df['Embedding'] = comments_df['Body'].apply(lambda x: get_glove_embedding(x, glove_model))

    # add embedding column to posts df by applying get_glove_embedding
    print('ADDING EMBEDDING COLUMN TO POSTS DF')
    posts_df['Post Text'] = posts_df['Post Text'].apply(preprocess_text)
    posts_df['Embedding'] = posts_df['Post Text'].apply(lambda x: get_glove_embedding(x, glove_model))

    comment_keywords_df = create_comment_keywords_df(comments_df)
    post_keywords_df = create_post_keywords_df(posts_df)


    x = pd.DataFrame(comments_df['Embedding'].tolist())

    # Silhouette Score for determining the best k
    sil_scores = []
    k = []
    for n_clusters in range(2, 11):
        kmeans = KMeans(n_clusters=n_clusters, random_state=0)
        kmeans.fit(x)
        sil_score = silhouette_score(x, kmeans.labels_)
        k.append(n_clusters)
        sil_scores.append(sil_score)

    # Selecting the best k
    best_k = k[sil_scores.index(max(sil_scores))]
    kmeans = KMeans(n_clusters=best_k, random_state=0).fit(x)

    # Assigning the labels to the comments
    labels = kmeans.labels_
    comment_keywords_df['Cluster'] = labels
    comments_df

    # Keyword frequecy per cluster
    keyword_frequency = {}

    for i in range(best_k):
        keyword_frequency['Cluster' + str(i)] = {}

        for keyword_list in comment_keywords_df['Keywords'][comment_keywords_df['Cluster'] == i]:
            for keyword in keyword_list:
                    if keyword[0] in keyword_frequency['Cluster' + str(i)].keys():
                        keyword_frequency['Cluster' + str(i)][keyword[0]] += keyword[1]
                    else:
                        keyword_frequency['Cluster' + str(i)][keyword[0]] = keyword[1]

        # Word cloud for visualizing the keywords in each cluster
        wordcloud = WordCloud(width=1920, height=1080, background_color='white').generate_from_frequencies(keyword_frequency['Cluster' + str(i)])
        wordcloud.to_file('data/Cluster' + str(i) + '.png')

    cluster_centers = kmeans.cluster_centers_
    body = comments_df['Body'].tolist()

    # Initialize a dictionary to store the points closest to each centroid
    closest_points = {i: [] for i in range(best_k)}

    # Find the n closest points to each centroid
    for i in range(len(x)):
        cluster_label = labels[i]
        centroid = cluster_centers[cluster_label]
        distance = np.linalg.norm(x.iloc[i] - centroid)
        closest_points[cluster_label].append((x.iloc[i], distance, body[i]))

    # Sort based on the distance to centroid
    n_closest = 4
    for i in range(best_k):
        closest_points[i] = sorted(closest_points[i], key=lambda y: y[1])[:n_closest]
        closest_points[i] = [[point[0], point[2]] for point in closest_points[i]]

    # Perform PCA for visualization (reduce to 2D)
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(x)
    cluster_centers_pca = pca.transform(cluster_centers)

    # Visualize the 10 closest points to each centroid
    plt.figure(figsize=(10, 6))
    for i in range(best_k):
        plt.scatter(X_pca[labels == i][:, 0], X_pca[labels == i][:, 1], label=f'Cluster {i}', alpha=0.8)
        plt.scatter(cluster_centers_pca[i, 0], cluster_centers_pca[i, 1], color='red', marker='X', s=200)

        # Annotate 10 closest points to centroid
        for point in closest_points[i]:
            plt.annotate(f'{point[1]}', (pca.transform([point[0]])[0]), textcoords="offset points", xytext=(0, 10), ha='center')

    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title('10 Closest Points to Each Centroid (PCA)')
    plt.legend()
    plt.savefig('data/messages.png')

    # write data to mysql database
    print('WRITING DATA TO MYSQL DATABASE')
    connection = create_connection()
    combined_comments_df = pd.concat([comments_df, comment_keywords_df], axis=1)
    combined_posts_df = pd.concat([posts_df, post_keywords_df], axis=1)
    if connection and connection.is_connected():
        # Insert into Posts table and get the IDs of the inserted rows
        for index, row in combined_posts_df.iterrows():
            post_vector = str(row['Embedding'])
            post_id = insert_post_metadata(connection, row.to_dict(), post_vector)
            if post_id:
                # Insert into Keywords table and corresponding Junction tables
                for keyword in row['Keywords']:
                    keyword_id = insert_keyword_get_id(connection, keyword)
                    query = "INSERT INTO PostKeywords (PostID, KeywordID) VALUES (%s, %s)"
                    execute_query(connection, query, (post_id, keyword_id))

        # Insert into Comments table and get the IDs of the inserted rows
        print('COMMENT DF ######################################')
        print(comment_keywords_df.head())

        for index, row in combined_comments_df.iterrows():
            comment_vector = str(row['Embedding'])
            comment_id = insert_comment_metadata(connection, comments_df.loc[index].to_dict(), comment_vector)

            if comment_id:
                # Insert into CommentKeywords table
                for keyword, frequency in row['Keywords']:
                    keyword_id = insert_keyword_get_id(connection, keyword)
                    if keyword_id is not None:
                        insert_into_junction_table(connection, 'CommentKeywords', comments_df.loc[index, 'postId'],
                                                   comment_id, keyword_id)

# Before running, manually create a database called "reddit"
nltk.download('punkt')
main()

# # Run these lines if needed before the code
# # Get the top posts from the subreddit
# print('GETTING TOP POSTS')
# posts_df = top_posts()

# # Get the comments for each post
# print('GETTING COMMENTS')
# comments_df = comments(posts_df['ID'])

# # Save the data to a CSV file
# posts_df.to_csv('data/posts.csv', index=False)
# comments_df.to_csv('data/comments.csv', index=False)

# # Get the GloVe model
# print('GETTING GLOVE MODEL')
# glove_model = get_glove_model()

# # add embedding column to comments df by applying get_glove_embedding
# print('ADDING EMBEDDING COLUMN TO COMMENTS DF')
# comments_df['Body'] = comments_df['Body'].apply(preprocess_text)
# comments_df['Embedding'] = comments_df['Body'].apply(lambda x: get_glove_embedding(x, glove_model))

# # add embedding column to posts df by applying get_glove_embedding
# print('ADDING EMBEDDING COLUMN TO POSTS DF')
# posts_df['Post Text'] = posts_df['Post Text'].apply(preprocess_text)
# posts_df['Embedding'] = posts_df['Post Text'].apply(lambda x: get_glove_embedding(x, glove_model))

# comment_keywords_df = create_comment_keywords_df(comments_df)
# post_keywords_df = create_post_keywords_df(posts_df)

# import numpy as np
# import matplotlib.pyplot as plt
# from wordcloud import WordCloud
# from sklearn.metrics import silhouette_score
# from sklearn.cluster import KMeans

# x = pd.DataFrame(comments_df['Embedding'].tolist())

# # Silhouette Score for determining the best k
# sil_scores = []
# k = []
# for n_clusters in range(2, 11):
#     kmeans = KMeans(n_clusters=n_clusters, random_state=0)
#     kmeans.fit(x)
#     sil_score = silhouette_score(x, kmeans.labels_)
#     k.append(n_clusters)
#     sil_scores.append(sil_score)

# # Selecting the best k
# best_k = k[sil_scores.index(max(sil_scores))]
# kmeans = KMeans(n_clusters=best_k, random_state=0).fit(x)

# # Assigning the labels to the comments
# labels = kmeans.labels_
# comment_keywords_df['Cluster'] = labels
# comments_df

# # Keyword frequecy per cluster
# keyword_frequency = {}

# for i in range(best_k):
#     keyword_frequency['Cluster' + str(i)] = {}

#     for keyword_list in comment_keywords_df['Keywords'][comment_keywords_df['Cluster'] == i]:
#          for keyword in keyword_list:
#                 if keyword[0] in keyword_frequency['Cluster' + str(i)].keys():
#                     keyword_frequency['Cluster' + str(i)][keyword[0]] += keyword[1]
#                 else:
#                     keyword_frequency['Cluster' + str(i)][keyword[0]] = keyword[1]

#     # Word cloud for visualizing the keywords in each cluster
#     wordcloud = WordCloud(width=1920, height=1080, background_color='white').generate_from_frequencies(keyword_frequency['Cluster' + str(i)])
#     wordcloud.to_file('data/Cluster' + str(i) + '.png')

# cluster_centers = kmeans.cluster_centers_
# body = comments_df['Body'].tolist()

# # Initialize a dictionary to store the points closest to each centroid
# closest_points = {i: [] for i in range(best_k)}

# # Find the n closest points to each centroid
# for i in range(len(x)):
#     cluster_label = labels[i]
#     centroid = cluster_centers[cluster_label]
#     distance = np.linalg.norm(x.iloc[i] - centroid)
#     closest_points[cluster_label].append((x.iloc[i], distance, body[i]))

# # Sort based on the distance to centroid
# n_closest = 4
# for i in range(best_k):
#     closest_points[i] = sorted(closest_points[i], key=lambda y: y[1])[:n_closest]
#     closest_points[i] = [[point[0], point[2]] for point in closest_points[i]]

# # Perform PCA for visualization (reduce to 2D)
# pca = PCA(n_components=2)
# X_pca = pca.fit_transform(x)
# cluster_centers_pca = pca.transform(cluster_centers)

# # Visualize the 10 closest points to each centroid
# plt.figure(figsize=(10, 6))
# for i in range(best_k):
#     plt.scatter(X_pca[labels == i][:, 0], X_pca[labels == i][:, 1], label=f'Cluster {i}', alpha=0.8)
#     plt.scatter(cluster_centers_pca[i, 0], cluster_centers_pca[i, 1], color='red', marker='X', s=200)

#     # Annotate 10 closest points to centroid
#     for point in closest_points[i]:
#         plt.annotate(f'{point[1]}', (pca.transform([point[0]])[0]), textcoords="offset points", xytext=(0, 10), ha='center')

# plt.xlabel('Principal Component 1')
# plt.ylabel('Principal Component 2')
# plt.title('10 Closest Points to Each Centroid (PCA)')
# plt.legend()
# plt.savefig('data/messages.png')

###AUTOMATION attempt

import time
import subprocess

def main():
  #getting an input for interval value on the command line
  interval_update_minutes = int(input("Enter the Update Interval in minutes: "))

  #scheduling of periodic updates
  while True:
    #Fetching Data
    print("Fetching Data...")
    subprocess.run(["python", "web_scraping_script.py"])  #web scraping file
    print("Data Fetched!")

    #Preprocessing Data
    print("Preprocessing Data...")
    subprocess.run(["python", "preprocess.py"])  #preprocessing file
    print("Data Preprocessed! ")

    #Database updation
    print("Updating the database...")
    subprocess.run(["python", "database_update.py"])    #database updation file
    print("Database Updated!")

    #Sleep for the specified period, before the next update
    print(f"Waiting for {interval_update_minutes} minutes... ")
    time.sleep(interval_update_minutes * 60) #converting the minutes into seconds


    #Checking if the user wants to quit
    user_input = input("enter 'quit' to stop update or press Enter to continue: ")
    if user_input.strip().lower() == 'quit':

      break


  print("Script Terminated!")


main()

